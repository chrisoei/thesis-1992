\documentstyle[12pt]{article}

\begin{document}

\quote{
Abstract:
The Walsh transform is used extensively as a tool in determining whether a
fitness function over a binary string is deceptive or not.  We will show that
the Walsh transform method for detecting deception is easily generalized to
functions over nonbinary strings.  We will also show a convenient way of
organizing the deceptive conditions using a generalization of Hadamard
transforms.
}

\section{Introduction}
One of the main topics of research in genetic algorithms is the study of
when the algorithm converges to the best answer.  Even if we know what
the best answer is, it is not easy to see whether or not the algorithm
converges to it.  One approach is to look at the average of the function
over similar strings; if the best string looks a lot like strings that
are generally bad, then the function is in some sense deceptive.
Unfortunately, taking these averages is computationally intensive.

In the past, Walsh functions have been used as a convenient way of
determining such quantities as schema averages and variances for binary
fitness functions.  In many cases, however, the fitness function takes a
nonbinary alphabet or even several real numbers.  For example, in real life,
DNA uses a base-4 alphabet.  In some engineering applications, a fitness
function may be defined over real numbers, such as the size of a particular
mechanical component in a design optimization problem.  We cannot analyze
these fitness functions by transforming them into fitness functions over
binary strings; doing so can introduce deception (such as Hamming cliffs)
due to the singularities in the transformation.  What we can do, however,
it to extend the Walsh function approach to cover these more general
cases as well.


\section{Functions over k-ary strings}
In this section, we look at functions over strings of length $n$ whose
characters are
taken from a k-ary alphabet.  For example, we can use this method to analyze
a function over a trinary alphabet string.  For convenience, we will
treat the string as if it were a two digit number in base 3.  We will use
this approach throughout the paper; all the characters in the alphabet
are assumed to be numbers in some base.

The basic idea is to treat each character in the string as a separate
dimension, then take the n-dimensional Fourier series.  The exact similarity
between the generalized Walsh transform and Fourier series will be discussed
later.  The point is that it is a useful to think of the Walsh functions
as functions of $n$ variables rather than functions of a single variable.

We define the k-ary Walsh function to be:
\begin{equation}
\Psi^{(k)}_{\vec j}(\vec x) = {\frac{1}{\sqrt{k^n}}
	e^{\frac{2 \pi \imath}{k} \vec x .\vec j }}
\end{equation}
where the vector $\vec j =(j_1,j_2,\ldots,j_n)$ is the k-ary representation
of $j$, and the vector $\vec x =(x_1,x_2,\ldots,x_n)$ is the k-ary 
representation of $x$.
Taking $k=2$ yields the usual Walsh functions, up to a normalization constant.
These functions satisfy the normalization condition:
\begin{equation}
\sum_{x=0}^{k^n-1}{\overline{\Psi^{(k)}_{\vec j}(\vec x)}
	\Psi^{(k)}_{\vec l}(\vec x)}=
	\delta_{j_1 l_1} \delta_{j_2 l_2} \ldots \delta_{j_n l_n}
\end{equation}
where $\overline{c}$ refers to the complex conjugate of $c$.

PROOF OF NORMALIZATION:
\begin{eqnarray}
\sum_{x=0}^{k^n-1}{\overline{\Psi^{(k)}_{\vec j}(\vec x)}
	\Psi^{(k)}_{\vec l}(\vec x)} \lefteqn \nonumber \\
	&= &\frac{1}{k^n}\sum_{x_1=0}^{k-1}{\sum_{x_2=0}^{k-1}{\ldots
		e^{- \frac{2 \pi \imath}{k} x_1 j_1}
		e^{\frac{2 \pi \imath}{k} x_1 l_1}
		e^{- \frac{2 \pi \imath}{k} x_2 j_2}
		e^{\frac{2 \pi \imath}{k} x_2 l_2}
		\ldots
		}} \nonumber \\
	&= & (\frac{1}{k}\sum_{x_1=0}^{k-1}
		{e^{\frac{2 \pi \imath}{k} x_1 (l_1-j_1)}})
	     (\frac{1}{k}\sum_{x_2=0}^{k-1}
		{e^{\frac{2 \pi \imath}{k} x_2 (l_2-j_2)}})
		\ldots \nonumber \\
	&=& \delta_{j_1 l_1} \delta_{j_2 l_2} \ldots \delta_{j_n l_n}
\end{eqnarray}

Let's do an example.  Take strings of length 2, $n=2$, using a trinary
alphabet, $k=3$.  Let's evaluate the fourth function, $\vec{j}=(1,1)$, at 
the second position, $\vec{x}=(0,2)$.
\begin{equation}
\Psi^{(3)}_{(1,1)}((0,2))=\frac{1}{\sqrt{3^2}}
	e^{\frac{2 \pi \imath}{3} (1,1).(0,2)}=
	-\frac{1}{6} - \frac{\sqrt{3}}{6} \imath
\end{equation}

Another way of looking at the Walsh functions $\Psi^{(\vec{j})}(\vec{x})$ 
is to think of 
them as (up to normalization) $e^{\imath \phi}$ where the phase $\phi$ 
is the inner product between the index of the function $\vec{j}$ and the
position $\vec{x}$ and the distance metric for the inner product is such
that the phase increases by $2 \pi$ as we traverse a dimension.

Now that we have the generalized Walsh functions, let us define the
generalized Walsh transform.  Simply put, a Walsh coefficient is the inner 
product of a Walsh function with the function.

\begin{equation}
w_j=\sum_x{\overline{\Psi^{(j)}(x)} f(x)}	\label{transform}
\end{equation}

Notice that we use the complex conjugate of the Walsh function.

Example:
Let's use a trinary alphabet again.
\begin{eqnarray}
w_{(0,1)} &=& \frac{1}{3} e^0 f((0,0)) \nonumber \\
	& &+ \frac{1}{3} e^{-2 \pi \imath /3} f((0,1)) \nonumber \\
	& &+ \frac{1}{3} e^{-4 \pi \imath /3} f((0,2)) \nonumber \\
	& &+ \frac{1}{3} e^0 f((1,0)) \nonumber \\
	& &+ \frac{1}{3} e^{-2 \pi \imath /3} f((1,1)) \nonumber \\
	& &+ \frac{1}{3} e^{-4 \pi \imath/3} f((1,2)) \nonumber\\
	& &+ \frac{1}{3} e^0 f((2,0)) \nonumber \\
	& &+ \frac{1}{3} e^{-2 \pi \imath /3} f((2,1)) \nonumber \\
	& &+ \frac{1}{3} e^{-4 \pi \imath /3} f((2,2))
\end{eqnarray}

The inverse transform is given by
\begin{equation}
f(x)=\sum_j{\Psi^{(x)}(j) w_j}	\label{inverse}
\end{equation}
Notice that the Walsh function is not conjugated in the inverse transform.

PROOF OF INVERSE TRANSFORM
We now show that the inverse transform of the transform of a function is
equal to itself.
\begin{eqnarray}
\sum_j{\Psi^{(x)}(j) w_j} &&\nonumber \\
&=& \sum_j{\Psi^{(x)}(j) \sum_y{\overline{\Psi^{(j)}(y)} f(y)}} \nonumber \\
&=& \sum_j{\Psi^{(x)}(j) \sum_y{\overline{\Psi^{(y)}(j)} f(y)}} \nonumber \\
&=& \sum_y{f(y) \sum_j {\Psi^{(x)}(j) \overline{\Psi^{(y)}(j)}}} \nonumber \\
&=& \sum_y{f(y) \delta_{xy}}\nonumber \\
&=& f(x)
\end{eqnarray}

We will wait until a later section to see in detail how these functions can be
used to analyze deception.  The procedure is a straightforward extension of
the binary case.  First, we use the orthogonality of the k-ary Walsh
functions to find k-ary Walsh coefficients for the function we want to analyze.
Then we use these coefficients to compute the schema averages, and use
the schema averages to say something about deception.

For now, we will state a few simple theorems about the transform to give a
more intuitive feeling for how it works:


Theorem:
A function is additively separable if $f(\vec{x})=f_1(x_1)+f_2(x_2)+\ldots$.
A function is additively separable if and only if its generalized
Walsh transform is also additively separable.

Proof:
The proof of this comes immediately from the fact that the transform is
linear.

Example:
Consider again the trinary alphabet and let $f((x_1,x_2))=3 x_1^2 + x_2$.
Let $f_1(x_1,x_2)=3 x_1^2$ and $f_2(x_1,x_2)=x_2$ so that 
$f(x_1,x_2)=f_1(x_1,x_2)+f_2(x_1,x_2)$.

\begin{verbatim}
f1  (0,0,0,3,3,3,12,12,12)
f2  (0,1,2,0,1,2,0,1,2)
f   (0,1,2,3,4,5,12,13,14)

w1= {15., 0, 0, -7.5 + 7.79423 I, 0, 0, -7.5 - 7.79423 I, 0, 0}
w2= {3., -1.5 + 0.866025 I, -1.5 - 0.866025 I, 0, 0, 0, 0, 0, 0}
w = {18., -1.5 + 0.866025 I, -1.5 - 0.866025 I, -7.5 + 7.79423 I, 0, 0, 
	-7.5 - 7.79423 I, 0, 0}
\end{verbatim}

Note that w1 + w2 = w.  Although this theorem is fairly trivial, it
stresses the idea that it is useful to think of the characters in the string
as being independent variables.  Also, functions which are partially
additively separable are often used in testing genetic algorithms.
Partially additively separable functions also arise in real applications.
For example, in designing a high-performance engine using genetic algorithms,
the first three characters in the string might code for the type of steel
used, while the next two characters might code for the fuel mixture.
Intuitively, these two characteristics are mostly independent; the
optimal fuel mixture does not depend strongly upon the type of steel used,
and the best kind of steel to use does not depend strongly upon the fuel
mixture.  We can express the ideas above mathematically as
\begin{equation}
f((x_1,x_2,x_3,x_4,x_5))=f_{123}(x_1,x_2,x_3) + f_{45}(x_4,x_5) +
	O(\epsilon)
\end{equation}
where $\epsilon$ is small compared to one, and all the functions 
$f, f_{123},$ and $f_{45},$ are of order one.  In this case,
\begin{equation}
w_{(j_1,j_2,j_3,j_4,j_5)}=u(j_1,j_2,j_3)+v(j_4,j_5) + O(\epsilon)
\end{equation}
More simply put: if the fitness function is well-approximated by a
partially additively separable function, then the Walsh transform is
also well-approximated by a partially additively separable function.


Theorem:
The average of the function keeping the first character fixed at $0$ and
varying the other characters is given by
\begin{equation}
\frac{1}{k^{n/2}}
(w_{(0,0,0,\ldots)}+w_{(1,0,0,\ldots)}+w_{(2,0,0,\ldots)}+\ldots+
	w_{(k-1,0,0,\ldots)})
\end{equation}
Note that $w_0$ is the average of the function (times a normalization
constant).

Notice that the summation over all the variables of the function except
the first character has been reduced to a
single summation in transform space.  This is why the Walsh transform is
useful.

Example:
Let us take $f((x_1,x_2))=3 x_1^2+x_2$ and use a ternary alphabet as in 
a previous example above.  Then the average of the function setting $x_1=0$
and letting $x_2$ vary is
\begin{equation}
\frac{1}{3}(f((0,0))+f((0,1))+f((0,2)))=\frac{1}{3}(w_{(0,0)}+w_{(1,0)}+w_{(2,0)})
\end{equation}
Had $f$ been a function of strings of length 3 instead of 2, the left hand
side of the above equation would have 9 terms, while the right hand side
would still have 3 terms.

Theorem:
The average of the square of the absolute value of the function is
equal to the average of the square of the absolute value of the Walsh
transform of the function.  This is analogous to the same relation in
Fourier transforms.  The usefulness of taking the average of $|f|^2$ will
become apparent in the section on the variance of fitness.

Proof:
\begin{eqnarray}
\sum_x{\overline{f(x)} f(x)}&=& 
	\sum_x{\overline{\sum_i{\Psi^{(x)}(i) w_i}}
		\sum_j{\Psi^{(x)}(j) w_j}} \nonumber \\
	&=& \sum_{ij}\sum_x {\overline{\Psi^{(x)}(i)}
		\Psi^{(x)}(j)
		\overline{w_i} w_j } \nonumber \\
	&=& \sum_{ij}\sum_x {\overline{\Psi^{(i)}{(x)}}
		\Psi^{(j)}(x)
		\overline{w_i} w_j } \nonumber \\
	&=& \sum_{ij} \delta_{ij} \overline{w_i} w_j \nonumber\\
	&=& \sum_i \overline{w_i} w_i
\end{eqnarray}

Example:
Let $f((x_1,x_2))= 3x_1^2+x_2$ as before.  The sum of squares of $f$ over
the entire $x$ space is 564, and so is the sum of the squares of the
absolute values of the walsh coefficients.

Theorem:
Instead of summing the squares of $f$ over the entire $x$ space, let us
sum it over just the strings with $x_1=0$.
\begin{eqnarray}
\sum_{x_2,x_3,\ldots,x_n}{\overline{f(x)} f(x)}&=&
        \sum_{x_2,x_3,\ldots,x_n}{\overline{\sum_i{\Psi^{(x)}(i) w_i}}
                \sum_j{\Psi^{(x)}(j) w_j}} \nonumber \\
        &=& \sum_{ij}\sum_{x_2,x_3,\ldots,x_n} {\overline{\Psi^{(x)}(i)}
                \Psi^{(x)}(j)
                \overline{w_i} w_j } \nonumber \\
        &=& \sum_{ij}\sum_{x_2,x_3,\ldots,x_n} {\overline{\Psi^{(i)}{(x)}}
                \Psi^{(j)}(x)
                \overline{w_i} w_j } \nonumber \\
        &=& \sum_{ij} \delta_{i_2 j_2} \delta_{i_3 j_3} \ldots
		\delta_{i_n j_n} \overline{w_i} w_j \nonumber \\
	&=& \sum_{i_1,j_1} \sum_{i_2,i_3,\ldots,i_n}
		\overline{w_i} w_{j_1,i_2,i_3,i_4,\ldots,i_n}
\end{eqnarray}
Notice that the sum on the left has $k^{n-1}$ terms, while the sum on
the right has $k^{n+1}$ terms.  So working in the transform space makes for
more work in this case.  But if the function $f$ is well-approximated by
a partially additively separable function, say $f(x)=f_1((x_1,\ldots,
x_{n/2})) + f_2((x_{n/2+1},\ldots,x_n))$ and $w=u+v$ with the same breakup, 
then the computation in transform space only involves approximately $k^{n/2}$ 
terms.
\begin{eqnarray}
\sum_{i_1,j_1} \sum_{i_2,i_3,\ldots,i_n}
	\overline{w_i} w_{j_1,i_2,i_3,i_4,\ldots,i_n} &=&
\sum_{i_1,j_1} \sum_{i_2,i_3,\ldots,i_n}
(\overline{u_{(i_1,i_2,\ldots,i_{n/2})}} 
+ \overline{v_{(i_{n/2+1},\ldots,i_{n})}}) \nonumber\\
 &&
(u_{(j_1,i_2,i_3,\ldots,i_{n/2})} + v_{(i_{n/2},\ldots,i_n)}) \nonumber \\
&=&
\sum_{i_1,j_1,i_2,i_3,\ldots,i_{n/2}} {
\overline{u_{(i_1,i_2,\ldots,i_{n/2})}} 
u_{(j_1,i_2,i_3,\ldots,i_{n/2})} } \nonumber \\
&&+
\sum_{i_{n/2+1},\ldots,i_n} {
\overline{v_{(i_{n/2+1},\ldots,i_{n})}}
v_{(i_{n/2+1},\ldots,i_n)}
} \nonumber\\
&&+
\sum_{i_1,i_2,\ldots,i_{n/2}} {
\overline{u_{(i_1,i_2,\ldots,i_{n/2})}} }
\sum_{i_{n/2},\ldots,i_n} {
v_{(i_{n/2+1},\ldots,i_n)} } \nonumber\\
&&
+\sum_{i_{n/2+1},\ldots,i_n} {
\overline{v_{(i_{n/2+1},\ldots,i_{n})}} }
\sum_{j_1,i_2,i_3,\ldots,i_{n/2}}
u_{(j_1,i_2,i_3,\ldots,i_{n/2})}
\end{eqnarray}
Of course, in this case the computation using a sum over $f^2$ also has 
about $k^{n/2}$ terms, so we have not gained anything by using the transform.

\section{Functions over more arbitrary strings}
In this section, we look at functions over strings of the form
$x_1 x_2 \ldots x_n$ where each $x_i$ is taken from a different alphabet
$\Sigma_i$.  

Let $k_i$ be the size of the alphabet $\Sigma_i$ and denote
them by $\vec k=(k_1,k_2,\ldots,k_n)$.
We now define the $\vec k$-ary Walsh functions to be:
\begin{equation}
\Psi^{\vec{k}}_{\vec j}(\vec{x})=
	\frac{1}{\sqrt{k_1 k_2 \ldots k_n}}
	\prod_{l=1}^n {exp(\frac{2 \pi \imath}{k_l} x_l j_l)}
\end{equation}
Again, we have the normalization:
\begin{equation}
\sum_{x=0}^{k_1 k_2 \ldots k_n -1}{ {\overline{\Psi^{(\vec{k})}_{\vec p}(\vec{x})}}
	\Psi^{(\vec{k})}_{\vec q}(\vec{x})}=
		\delta_{p_1 q_1} \delta_{p_2 q_2} \ldots
		\delta_{p_n q_n}
\end{equation}
The proof of the normalization works exactly the same way as it did
before.  The form of the transform (\ref{transform}) and inverse transform 
(\ref{inverse}) also remain the same.

\section{Generalizing the Fast Walsh Transform}
The Fast Walsh Transform can be generalized in a similar manner.
In the ordinary Fast Walsh Transform, we place the function values $f(x)$ on a
binary tree; the position of $f(x)$ on the tree corresponds to the
representation of $x$.  For example, $f((0,1,0,0))$ would be found by
starting at the root, taking the left branch, then the right, then a
left, and finally another left.

Now we descend down and process the tree level by level.  At each level,
we apply the algorithm described below to each node in that level before
descending down to the next level.  The algorithm is the following:
take the left subtree $l$ of the node, add it to the right subtree
of the node, and call the result $l'$; take the left subtree $l$ of
the node, subtract the right subtree $r$  and call it $r'$.  Now replace
the left subtree with $l'$ and the right subtree with $r'$.
More briefly: $(l,r) \rightarrow (l + r,l - r)$ where the tree-wise
addition and subtraction means to add like components of $l$ and $r$.

To generalize this to functions over a k-ary alphabet, we form a $k$-ary
tree and descend level by level.  Instead of applying the rule
$(l,r) \rightarrow (l+r,l-r)$, we apply
\begin{eqnarray}
\lefteqn (c_1,c_2,\ldots,c_k) &\rightarrow& \nonumber \\
& &(c_1+c_2+\ldots+c_k,\nonumber \\
& &c_1+e^{\frac{2 \pi \imath}{k}} c_2 + e^\frac{4 \pi \imath}{k} c_3 + \ldots 
	+ e^\frac{2 (k-1) \pi \imath}{k} c_k, \nonumber \\
& &\cdots)
\end{eqnarray}
The basic idea is the same as with the ordinary Fast Walsh Transform, but
instead of simply adding and subtracting the children at each node, we perform
a Fourier transform upon the children.
More briefly: $c \rightarrow FT[c]$.  Note that if we have a large alphabet,
it is worthwhile to perform this Fourier transform using a Fast Fourier
transform.

Note that if we have a $\vec{k}$-ary alphabet, then we can still perform
the Fast Walsh Transform.  The root node of our tree would have $k_1$
children; all the nodes at the next lower level would have $k_2$ children,
etc.  We still go down the tree level by level and apply the Fourier
transform upon the children of each node.

\section{Using generalized Walsh coefficients to determine schema averages}
Determining schema averages is at the heart of the reason of using these
transform methods.  Previously, we noted that a sum over $m$ characters
of a string of length $n$ turned into a sum over $n-m$ characters when
we used the Walsh transform.  For this reason, the average of a function
over a schema with $m$ positions fixed turns into a sum over $m$ characters
of the Walsh coefficients.

This is best demonstrated by example.  Let us consider strings of length
3 taken from a (2,4,2)-ary alphabet.
\begin{eqnarray}
f((*,*,*)) &=& \frac{1}{4} w_{(0,0,0)} \nonumber \\
f((0,*,*)) &=& \frac{1}{4} (w_{(0,0,0)}+w_{(1,0,0)}) \nonumber \\
f((1,*,*)) &=& \frac{1}{4} (w_{(0,0,0)}-w_{(1,0,0)}) \nonumber \\
f((0,0,*)) &=& \frac{1}{4} (w_{(0,0,0)}+
				w_{(0,1,0)}+
				w_{(0,2,0)}+
				w_{(0,3,0)}+
				w_{(1,0,0)}+
				w_{(1,1,0)}+
				w_{(1,2,0)}+
				w_{(1,3,0)}) \nonumber \\
f((*,0,*)) &=& \frac{1}{4} (w_{(0,0,0)}+
				w_{(0,0,1)}+
				w_{(1,0,0)}+
				w_{(1,0,1)})
\end{eqnarray}
Note that the difference between the sums for $f((0,*,*))$ and $f((1,*,*))$
is that the Walsh coefficients are subtracted rather than added.  In
general, when we have a fixed character $p$ in the j-th position in the schema,
the corresponding sum in the transform has a phase of $e^{2 \pi p/k_j}$.
Again, a few examples:
\begin{eqnarray}
f((*,0,*)) &=& \frac{1}{4} (w_{(0,0,0)}+
				w_{(0,0,1)}+
				w_{(1,0,0)}+
				w_{(1,0,1)}) \nonumber \\
f((*,1,*)) &=& \frac{1}{4} (w_{(0,0,0)}+ \imath
				w_{(0,0,1)}-
				w_{(1,0,0)} - \imath
				w_{(1,0,1)}) \nonumber \\
f((*,2,*)) &=& \frac{1}{4} (w_{(0,0,0)}-
				w_{(0,0,1)}+
				w_{(1,0,0)} -
				w_{(1,0,1)}) \nonumber \\
f((*,3,*)) &=& \frac{1}{4} (w_{(0,0,0)}-\imath
				w_{(0,0,1)}+
				w_{(1,0,0)} +\imath
				w_{(1,0,1)}) 
\end{eqnarray}

It is easy to make a general theorem from these observations.
Let us say a schema has $m$ fixed characters $p_i$ at positions $j_i$.
Then the average of $f$ over that schema is
\begin{eqnarray}
\prod_q{k_q^{-1/2}}
\sum_{l_1} \sum_{l_2} \ldots \sum_{l_m} & &
e^{2 \pi \imath (l_1 p_1/k_{j_1}+l_2 p_2/k_{j_2} 
	+\ldots+l_m p_m/k_{j_m})}\nonumber\\
&& w_{(0,0,\ldots,0,l_1,0,\ldots,0,l_2,0,\ldots,0,l_m,0,\ldots)}
\end{eqnarray}

PROOF:
Recall that when we write $f$ in terms of its Walsh coefficients, the
result has $n$ sums, where $n$ is the length of the strings.  There is
a sum for each of the $n$ characters in the string of the argument.  Now,
when we take the average of $f$ over a schema which has a $*$ in the j-th
position, only the terms that have no phase change as we traverse the j-th
dimension survive; this means that only the Walsh coefficients with a $0$
in the j-th position survive in the final result.


Let us do one more example.  Consider a function $f$ over strings of length
2 taken from a (3,2)-ary alphabet.
\begin{eqnarray}
f((*,*))&=& \frac{1}{6} w_{(0,0)} \nonumber \\
f((*,0))&=& \frac{1}{6} (w_{(0,0)}+w_{(0,1)}) \nonumber \\
f((*,1))&=& \frac{1}{6} (w_{(0,0)}-w_{(0,1)}) \nonumber \\
f((0,*))&=& \frac{1}{6} (w_{(0,0)}+w_{(1,0)}+w_{(2,0)}) \nonumber \\
f((1,*))&=& \frac{1}{6} (w_{(0,0)}+e^{2 \pi \imath/3} w_{(1,0)} +
				e^{4 \pi  \imath/3} w_{(2,0)}) \nonumber \\
f((2,*))&=& \frac{1}{6} (w_{(0,0)}+e^{4 \pi \imath/3} w_{(1,0)} +
				e^{8 \pi \imath/3} w_{(2,0)}) \nonumber \\
f((1,1))&=& \frac{1}{6} (w_{(0,0)}-w_{(0,1)} + e^{2 \pi \imath/3} w_{(1,0)}
			+ e^{- \pi \imath/3} w_{(1,1)} 
			+ e^{-2 \pi \imath/3} w_{(2,0)}
			+ e^{\pi \imath/3} w_{(2,1)})
\end{eqnarray}

\section{Other useful basis for transforms}

As in Fourier series, we can use sines and cosines as well as complex
exponentials as our basis functions.  The disadvantage of using sines
and cosines as basis functions is that the basic theorems become somewhat
more complex.  However, the advantage is that all the Walsh coefficients
of a real function are real in this basis.  More importantly, this
new basis gives us a way of understanding some more results about schema
averages over nonbinary strings.

Since $\cos(x)=\frac{e^{\imath x}+e^{-\imath x}}{2}$ and
$\sin(x)=\frac{e^{\imath x}-e^{-\imath x}}{2 \imath}$,
we can convert between the Walsh coefficients we used in the previous
sections to the Walsh coefficients using sines and cosines.

To start, let us consider a function over strings of length 1.  The
inverse transform using sines and cosines will have the form:
There are two cases for the transform itself, one for even k and one for odd k.

ODD K:
\begin{eqnarray}
a_j &=& w_j + w_{k-j} \nonumber \\
a_0 &=& w_0 \nonumber \\
b_j &=& \imath (w_j - w_{k-j})
\end{eqnarray}
\begin{equation}
f(x)=\frac{1}{k^{1/2}}(a_0+\sum_{j=1}^{\frac{k-1}{2}}{(a_j \cos(2 \pi j x/k) +
	 b_j \sin(2 \pi j x/k))})
\end{equation}
Of course, we can express the $a_j$ and $b_j$ in terms of the inner products
of the cosines and sines with the function $f$.
\begin{eqnarray}
a_j &=& \frac{2}{k^{1/2}} \sum_{x=0}^{k-1} {\cos(2 \pi j x/k) f(x)} \nonumber \\
b_j &=& \frac{2}{k^{1/2}} \sum_{x=0}^{k-1} {\sin(2 \pi j x/k) f(x)}
\end{eqnarray}

EVEN K:
\begin{eqnarray}
a_j &=& w_j + w_{k-j} \nonumber \\
a_0 &=& w_0 \nonumber \\
a_{k/2} &=& w_{k/2} \nonumber \\
b_j &=& \imath (w_j - w_{k-j})
\end{eqnarray}
\begin{equation}
f(x)=\frac{1}{k^{1/2}}(a_0+\sum_{j=1}^{\frac{k}{2}-1}{(a_j \cos(2 \pi j x/k) + 
	b_j \sin(2 \pi j x/k))}
	+\cos(\pi x) a_{k/2})
\end{equation}
\begin{eqnarray}
a_j &=& \frac{2}{k^{1/2}} \sum_{x=0}^{k-1} {\cos(2 \pi j x/k) f(x)} \nonumber \\
b_j &=& \frac{2}{k^{1/2}} \sum_{x=0}^{k-1} {\sin(2 \pi j x/k) f(x)}
\end{eqnarray}

The proof of these cases follows simply from substituting the expressions
for $a_j$ and $b_j$ into the expressions for $f(x)$ and verifying that it
indeed is the inverse Walsh transform (\ref{inverse}).

There are two ways of generalizing the above method to $n$ dimensions.
The first is simply to take the Fourier transform along each dimension
as we did before, but to use sines and cosines as we have done above.
The problem with using this method is mainly notational complexity, as
we will see in the following example, although the idea is just as simple
as the generalized Walsh transforms we discussed before.  All we are
doing is taking the Fourier transform in an $n$-dimensional space, using
sines and cosines.

Consider a function over strings of length 2 whose characters are taken
from a ternary alphabet.

\begin{eqnarray}
aa_{(0,0)} &=& w_{(0,0)} \nonumber \\
aa_{(0,m)} &=& w_{(0,m)} + w_{(0,k-m)} \nonumber \\
aa_{(l,0)} &=& w_{(l,0)} + w_{(k-l,0)} \nonumber \\
ab_{(0,m)} &=& w_{(0,m)} - w_{(0,k-m)} \nonumber \\
ba_{(l,0)} &=& w_{(l,0)} + w_{(k-l,0)} \nonumber \\
aa_{(l,m)} &=& w_(l,m) + w_(l,k-m) + w_(k-l,m) + w_(k-l,k-m) \nonumber \\
ab_{(l,m)} &=& w_(l,m) - w_(l,k-m) + w_(k-l,m) - w_(k-l,k-m) \nonumber \\
ba_{(l,m)} &=& w_(l,m) + w_(l,k-m) - w_(k-l,m) - w_(k-l,k-m) \nonumber \\
bb_{(l,m)} &=& w_(l,m) - w_(l,k-m) - w_(k-l,m) + w_(k-l,k-m) \nonumber \\
\end{eqnarray}
\begin{eqnarray}
f((x_1,x_2))  &=& \frac{1}{3}( \sum_{l=0}^{\frac{k-1}{2}} {
	\sum_{m=0}^{\frac{k-1}{2}} {}}\nonumber \\
&+& aa_{(l,m)} \cos(2 \pi \imath l x_1/k) \cos(2 \pi \imath l x_2/k)\nonumber \\
&+& ab_{(l,m)} \cos(2 \pi \imath l x_1/k) \sin(2 \pi \imath l x_2/k)\nonumber\\
&+& ba_{(l,m)} \sin(2 \pi \imath l x_1/k) \cos(2 \pi \imath l x_2/k)\nonumber\\
&+& bb_{(l,m)} \sin(2 \pi \imath l x_1/k) \sin(2 \pi \imath l x_2/k)
)
\end{eqnarray}

Clearly, this first way of generalizing the sine and cosine transform becomes 
cumbersome for long strings, and so we will not pursue it any further.

The second method of generalizing the Walsh
transform using sines and cosines works as follows:
\begin{eqnarray}
a_{\vec{0}} &=& \prod_m{k_m^{-1/2}} \sum_{\vec{x}} {f(\vec{x})} \nonumber\\
a_{\vec{\jmath}} &=& 2 \prod_m{k_m^{-1/2}} \sum_{x_1,x_2,\ldots,x_n}
	{\cos(2 \pi (j_1 x_1/k_1+j_2 x_2/k_2+\ldots j_n x_n/k_n)
	f(\vec{x})} \nonumber\\
b_{\vec{\jmath}} &=& 2 \prod_m{k^{-1/2}} \sum_{x_1,x_2,\ldots,x_n}
	{\cos(2 \pi (j_1 x_1/k_1+j_2 x_2/k_2+\ldots j_n x_n/k_n)
	f(\vec{x})}
\end{eqnarray}
\begin{eqnarray}
f(\vec{x}) &=& \prod_m{k_m^{-1/2}} \sum_{\vec{\jmath}}
{}( 
a_{\vec{\jmath}} \cos(2 \pi (j_1 x_1/k_1+j_2 x_2/k_2+\ldots j_n x_n/k_n)) 
	\nonumber\\
&& +
b_{\vec{\jmath}} \sin(2 \pi (j_1 x_1/k_1+j_2 x_2/k_2+\ldots j_n x_n/k_n))
)
\end{eqnarray}
This second method has the disadvantage that the Walsh functions in $n$
dimensions is not just the product of Walsh functions in one dimension.
Explicitly, they are the following:
\begin{eqnarray}
\Omega_{\vec{\jmath}} &=& \prod_m{k_m^{1/2}} \cos(2 \pi (j_1 x_1/k_1 +
	j_2 x_2/k_2 + \ldots j_n x_n /k_n)) \nonumber\\
\Lambda_{\vec{\jmath}} &=& \prod_m{k_m^{1/2}} \sin(2 \pi (j_1 x_1/k_1 +
	j_2 x_2/k_2 + \ldots j_n x_n /k_n))
\end{eqnarray}

This transform also has the useful property that function averages over schema
with $m$ fixed positions become sums over $n-m$ positions in the transformed
space.  For example, let us consider again a function $f$ over strings of
length 2 whose characters are taken from a trinary alphabet.  Some schemas
averages in terms the transform coefficients are:
\begin{eqnarray}
f(**) &=& \frac{1}{3} a_{(0,0)} \nonumber \\
f(*0) &=& \frac{1}{3} (a_{(0,0)} + a_{(0,1)}) \nonumber\\
f(*1) &=& \frac{1}{3} (a_{(0,0)} -\frac{1}{2} a_{(0,1)} +\frac{3^{1/2}}{2} b_{(0,1)}) \nonumber\\
f(*2) &=& \frac{1}{3} (a_{(0,0)} -\frac{1}{2} a_{(0,1)} -\frac{3^{1/2}}{2} b_{(0,1)})
\end{eqnarray}

Let us say a schema has $m$ fixed characters $p_i$ at positions $j_i$.
Then the average of $f$ over that schema is
\begin{eqnarray}
\prod_q{k_q^{-1/2}}
\sum_{l_1} \sum_{l_2} \ldots \sum_{l_m} & &
\cos(2 \pi (l_1 p_1/k_{j_1}+l_2 p_2/k_{j_2} +\ldots+l_m p_m/k_{j_m}))\nonumber\\
&& a_{(0,0,\ldots,0,l_1,0,\ldots,0,l_2,0,\ldots,0,l_m,0,\ldots)}\nonumber\\
&+&
\sin(2 \pi (l_1 p_1/k_{j_1}+l_2 p_2/k_{j_2} +\ldots+l_m p_m/k_{j_m}))\nonumber\\
&& b_{(0,0,\ldots,0,l_1,0,\ldots,0,l_2,0,\ldots,0,l_m,0,\ldots)}
\end{eqnarray}

\section{Functions over reals}
As a computer internally represents real numbers as integers, we might expect
that there should be some similarity between analyzing genetic
algorithms whose fitness functions are defined over
strings whose characters are taken over large alphabets and analyzing
GAs with fitness functions over reals.

In the beginning, we discussed how the generalized Walsh transform
was equivalent to taking the Fourier transform along each dimension.
All we need to do is apply this exact same idea.  If the fitness function
is a function of two real variables and three discrete variables, then
we simply take the Fourier transform along all five dimensions. 
To avoid normalization constants, let us assume that the real variables
of the fitness function, $x_1$ and $x_2$, are always in the unit interval
[0,1].  Later,
we will discuss how to analyze functions whose variables are in the range
$[0,\infty]$ and $[-\infty,\infty]$.  The discrete variables of the
function, $x_3$, $x_4$, and $x_5$ are taken from a ternary alphabet.
Then the corresponding generalized Walsh coefficients are:
\begin{equation}
w_{(j_1,j_2,j_3,j_4,j_5)} = 3^{-3/2}
\int_0^1{dx_1 \int_0^1{dx_2 \sum_{x_3=0}^2{\sum_{x_4=0}^2{\sum_{x_5=0}^2{f(\vec{x})
	e^{-2 \pi \imath (x_1 j_1 + x_2 j_2 + x_3 j_3/3 + x_4 j_4/3 + x_5 j_5/3)}}}}}} 
\end{equation}
And the inverse transform is:
\begin{equation}
f(\vec{x})= 3^{-3/2}
\sum_{j_1=-\infty}^{\infty}{ \sum_{j_2=-\infty}^{\infty}{
	\sum_{j_3=0}^2{ \sum_{j_4=0}^2{ \sum_{j_5=0}^2 {
	w_{(j_1,j_2,j_3,j_4,j_5)}
	e^{2 \pi \imath (x_1 j_1 + x_2 j_2 + x_3 j_3/3 + x_4 j_4/3 + x_5 j_5/3)}}}}}} 
\end{equation}
Notice that the sum over $j_1$ and $j_2$ run from $-\infty$ to $\infty$,
and that there is no normalization factor associated with $x_1$ and $x_2$.

We can also define schemas in this space, and take schema averages using
these generalized Walsh coefficients as before.  For example, let
(0.67,*,*,*,1) refer to the set of strings whose first variable $x_1$
equals 0.67 and last variable $x_5$ equals 1.  Then the average of the
function $f$ over that schema is
\begin{equation}
f((0.67,*,*,*,1))=3^{-3/2}\sum_{j_1=-\infty}^{\infty} {\sum_{j_5=0}^2{
	w_{(j_1,0,0,0,j_5)} e^{2 \pi \imath (0.67 j_1 + 1 j_5/3)}
}}
\end{equation}
To get more familiar with using these generalized Walsh coefficients on
real variables, let us do some more schema averages:
\begin{eqnarray}
f((*,*,*,*,*)) &=& 3^{-3/2} w_{(0,0,0,0,0)} \nonumber\\
f((*,*,*,*,2)) &=& 3^{-3/2} (w_{(0,0,0,0,0)} + 
	e^{2 \pi \imath/3} w_{(0,0,0,0,1)} + 
	e^{4 \pi \imath/3} w_{(0,0,0,0,2)}) \nonumber\\
f((0,*,*,*,*)) &=& 3^{-3/2} \sum_{j_1=-\infty}^{\infty}{w_{(j_1,0,0,0,0)}}
\end{eqnarray}
These schemas over real variables are analogous to the ``slices'' used
in \cite{real}.

One of the most useful properties about using these generalized Walsh
transforms is that functions over reals or large alphabets can sometimes
be well-approximated by functions over small alphabets.  
Consider a function over a single real variable.
It is well-known that
if the first $m$ derivatives of the function are continuous, and the $m+1$-th
derivative is discontinuous, then for large
$j$, the magnitude of the Fourier coefficients fall as $j^{-m-2}$.
Thus, if the function is smooth enough, then we can get good approximations
to schema averages by dropping the generalized Walsh coefficients with
high spatial frequencies.

As an example of this approximation, let us consider the following test
function:
\begin{equation}
f(x)= \left\{ \begin{array}{ll}
	x & \mbox{if $x<1/2$} \\
	0 & \mbox{otherwise}
	\end{array}
	\right.
\end{equation}
This function has no continuous derivatives, and is discontinuous itself.
Thus, the Fourier coefficients $w_j$ fall as $j^{-1}$.  See the figure
for ``Test Function 1'' for comparisons of the function with the approximations.
Notice that since $f$ is real-valued, the Fourier coefficients $w_j$ and
$w_{-j}$ are complex conjugates; and to get a real-valued approximation,
if we keep $w_j$ in our sum, we must also keep $w_{-j}$.

\begin{equation}
f(x)=x^2(1-x)^2 (\frac{1}{2}-x)^3
\end{equation}
Since $\partial^2 f/\partial x^2$ equals $1/4$ at $x=0$ and $-1/4$ at $x=1$,
this function has a discontinuous second derivative.  Therefore, the
Fourier coefficients fall as $j^{-3}$.  See the figure for ``Test Function 2''
for comparisons of this function with the approximations.  Notice how
much faster the approximations converge to this function than the previous
one.

If a k-term Fourier approximation is satisfactory, then we can treat
the function as if it were a function over a k-ary alphabet.  Not only
does this make taking schema averaging simple, but it also makes
determining deception much easier.  As an example, let us consider a
function $f((x_1,x_2))$ where $x_1$ is a real variable in [0,1] and
$x_2$ is a character from a ternary alphabet.  Let $w_{(j_1,j_2)}$ be
the generalized Walsh transform of $f$.  Let $w'_{(j_1,j_2)}$ be the
(5,2)-ary approximation of $w$.  Then
\begin{eqnarray}
w'_{(0,j_2)} &=& 5^{1/2} w_{(0,j_2)} \nonumber \\
w'_{(1,j_2)} &=& 5^{1/2} w_{(1,j_2)} \nonumber \\
w'_{(2,j_2)} &=& 5^{1/2} w_{(2,j_2)} \nonumber \\
w'_{(3,j_2)} &=& 5^{1/2} w_{(-2,j_2)} \nonumber \\
w'_{(4,j_2)} &=& 5^{1/2} w_{(-1,j_2)}
\end{eqnarray}
The reason this works is that k-ary generalized Walsh functions are
periodic with period k, and therefore the Walsh coefficients with indices
$-m$ correspond to Walsh coefficients with indices $k-m$.  The mapping
we used above keeps the $5\times 2$ Walsh coefficients with the lowest spatial
frequency and fixes the normalization.

Similarly, if we had a function over a (100,2)-ary alphabet and we wanted
to reduce it to a function over a (7,2)-ary alphabet, we would use
\begin{eqnarray}
w'_{(0,j_2)} &=& \frac{7^{1/2}}{100^{1/2}} w_{(0,j_2)} \nonumber \\
w'_{(1,j_2)} &=& \frac{7^{1/2}}{100^{1/2}} w_{(1,j_2)} \nonumber \\
w'_{(2,j_2)} &=& \frac{7^{1/2}}{100^{1/2}} w_{(2,j_2)} \nonumber \\
w'_{(3,j_2)} &=& \frac{7^{1/2}}{100^{1/2}} w_{(3,j_2)} \nonumber \\
w'_{(4,j_2)} &=& \frac{7^{1/2}}{100^{1/2}} w_{(97,j_2)} \nonumber \\
w'_{(5,j_2)} &=& \frac{7^{1/2}}{100^{1/2}} w_{(98,j_2)} \nonumber \\
w'_{(6,j_2)} &=& \frac{7^{1/2}}{100^{1/2}} w_{(99,j_2)} \nonumber \\
\end{eqnarray}
Again, we simply take the $7\times 2$ Walsh coefficients with the lowest spatial
frequency and fix the normalization.

Let us do one final example.  Consider again the ``Test Function 2'' we
used earlier.  The Walsh coefficients are
\begin{eqnarray}
w_0&=&0  \nonumber\\
w_1&=& -0.000265108 \imath \nonumber\\
w_{-1} &=& 0.000265108 \imath \nonumber\\
w_2&=& -0.000124861 \imath \nonumber \\
w_{-2}&=& 0.000124861 \imath \nonumber\\
w_3 &=& -0.0000175818 \imath  \nonumber\\
w_{-3} &=& 0.0000175818 \imath \nonumber\\
\vdots & & \vdots
\end{eqnarray}
The 7-ary approximation to ``Test Function 2'' would have Walsh coefficients
$w'$, where
\begin{eqnarray}
w'_0 &=& 7^{1/2} w_0 = 0\nonumber \\
w'_1 &=& 7^{1/2} w_1 = -0.000701409 \imath \nonumber\\
w'_2 &=& 7^{1/2} w_2 = -0.000330351 \imath \nonumber\\
w'_3 &=& 7^{1/2} w_3 = 0.0000465171 \imath \nonumber \\
w'_4 &=& 7^{1/2} w_{-3} = -0.0000465171 \imath \nonumber \\
w'_5 &=& 7^{1/2} w_{-2} = 0.000330351 \imath \nonumber\\
w'_6 &=& 7^{1/2} w_{-1} = 0.00070149 \imath
\end{eqnarray}

These concepts of virtual alphabets are also used in \cite{real}.

functions over the entire real line[-infinity,infinity]
functions over half the real line [0,infinity] (laplace transform?)


\section{Using generalized Hadamard transforms to detect deception}
Hadamard transforms provide a convenient way of checking
deceptive conditions in a systematic manner \cite{hadamard}.

Deception in functions over nonbinary strings is qualitatively different than
deception in functions over binary strings.  In order to get full deception
with binary strings, all schemas of order $n-1$ or less must point towards
the complement of the global optimum.  With nonbinary strings, all that
is required is that all schemas of order $n-1$ or less point away from the
global optimum.

For example, let us consider a function $f$ over a ternary string of length
3 whose global optimum is at (2,2,2).  Then some of the conditions necessary
for deception look like:
\begin{eqnarray}
f((2,*,*)) &<& f((0,*,*)) \mbox{or} f((1,*,*)) \nonumber\\
f((2,2,*)) &<& f((0,0,*)) \mbox{or} f((0,1,*)) \mbox{or} f((1,0,*)) \mbox{or} f((1,1,*)) \nonumber\\
f((2,2,*)) &<& f((1,2,*)) \mbox{or} f((0,2,*)) \nonumber \\
f((2,2,*)) &<& f((2,0,*)) \mbox{or} f((2,1,*)) \nonumber \\
f((1,2,*)) &<& f((1,0,*)) \mbox{or} f((1,1,*))
\end{eqnarray}

Mutation for nonbinary strings works differently than mutation for binary
strings.  Some mutation operators, such as creeping mutation, mutate
characters to nearby characters.  For example, using a 10-ary alphabet,
the string (0,0,0) is much more likely to be perturbed to (1,0,0) than (9,0,0).
Similarly, mutations on a real number might be implemented by adding noise
with a gaussian distribution.  If the fitness function is not too
discontinuous, then these kinds of mutations can work as a sort of gradient
descent to help convergence.  Thus, it is reasonable to define a fully
deceptive function as a function whose order $n-1$ or less schemas point as far
away from the global optima as possible.  Now the deceptive conditions
become more straightforward:
\begin{eqnarray}
f((0,*,*)) &>& f((1,*,*)) \nonumber\\
f((0,*,*)) &>& f((2,*,*)) \nonumber\\
f((0,0,*)) &>& f((0,1,*)) \nonumber\\ 
f((0,0,*)) &>& f((0,2,*)) \nonumber\\ 
f((0,0,*)) &>& f((1,0,*)) \nonumber\\ 
f((0,0,*)) &>& f((1,1,*)) \nonumber\\ 
f((0,0,*)) &>& f((1,2,*)) \nonumber\\
f((0,0,*)) &>& f((2,0,*)) \nonumber\\
f((0,0,*)) &>& f((2,1,*)) \nonumber\\
f((0,0,*)) &>& f((2,2,*))
\end{eqnarray} 
and all permutations of the strings above.

Let us consider the competition partition in which the fixed positions
are at $j_1,j_2,\ldots,j_p$, where $p$ is the order of the partition.
We define the generalized Hadamard transform matrix $H$ as: 
\begin{equation} 
H=h_1 \otimes h_2 \otimes \ldots \otimes h_p
\end{equation}
where $\otimes$ is the tensor product:
\begin{equation}
\left(\begin{array}{cc}
	a & b\\
	c & d
	\end{array} \right)
\otimes \left(\begin{array}{cc}
	e & f\\
	g &h
	\end{array} \right)
= \left(\begin{array}{cccc}
	ae& af& be& bf\\
	ag& ah& bg& bh \\
	ce& cf& de& df \\
	cg& ch& dg& dh
\end{array}\right)
\end{equation}
and $h_m$ is defined as:
\begin{equation}
h_m = \left(\begin{array}{ccccc}
		1 & 1& 1& \ldots &1 \\
		1 & e^{2 \pi \imath /k_{j_m}} & e^{4 \pi \imath/k_{j_m}} &\ldots &
			e^{(k_{j_m}-1) 2 \pi \imath/k_{j_m}} \\
		\vdots &\vdots &\vdots &\ddots &\vdots \\
		1 & e^{(k_{j_m}-1) 2 \pi \imath/k_m} &
			e^{(k_{j_m}-1) 4 \pi \imath/k_m} &\ldots&
			e^{(k_{j_m}-1) (k_{j_m}-1) 2 \pi \imath/k_{j_m}}
	\end{array}
	\right)
\end{equation}

Now we make the important assumption that the global optimum is at
$(k_1-1,k_2-1,\ldots,k_n-1)$ and so all schemas of order $n-1$ or less
must point to $(0,0,\ldots,0)$.  Using this fact, we can now define the
matrix $M$ such that the deceptive conditions for that partition become
$M W>0$, where
\begin{equation}
M= \left(\begin{array}{c}
	\mbox{1st row of H-2nd row of H} \\
	\mbox{1st row of H-3rd row of H} \\
	\vdots \\
	\mbox{1st row of H-last row of H}
	\end{array}
	\right)
\end{equation}
and $W$ is the vector of generalized Walsh coefficients used in the
competition partition.

For example, let $w$ be the Walsh coefficients for the fitness function $f$
over (3,2,2)-ary strings.  Consider the competition partition $(F,*,*)$, where
$F$ represents a fixed position and * represents a wildcard character.  Then
\begin{eqnarray}
W&=&(w_{(0,0,0)},w_{(1,0,0)},w_{(2,0,0)})^T \nonumber\\
H&=&\left(\begin{array}{ccc}
	1&1&1\\
	1&e^{2 \pi \imath/3}&e^{4 \pi \imath/3}\\
	1&e^{4 \pi \imath/3}&e^{8 \pi \imath/3}
	\end{array}
	\right) \nonumber\\
M&=&\left(\begin{array}{ccc}
	0&1-e^{2 \pi \imath/3}&1-e^{4 \pi \imath/3} \\
	0&1-e^{4 \pi \imath/3}&1-e^{8 \pi \imath/3}
	\end{array}
	\right)
\end{eqnarray}
The condition that $MW>0$ becomes:
\begin{eqnarray}
(1-e^{2\pi \imath/3})w_{(1,0,0)}+(1-e^{4\pi\imath/3})w_{(2,0,0)} &>&0\nonumber\\
(1-e^{4\pi \imath/3})w_{(1,0,0)}+(1-e^{8\pi\imath/3})w_{(2,0,0)} &>&0
\end{eqnarray}
When we substitute function values for the Walsh coefficients in the above
expression, we get
\begin{eqnarray}
f((0,*,*)) &>& f((1,*,*)) \nonumber\\
f((0,*,*)) &>& f((2,*,*))
\end{eqnarray}
which is indeed the deceptive conditions corresponding to that partition.

Let us repeat the above calculations for the competition partition
$(F,D,F)$.
\begin{eqnarray}
W&=&(w_{(0,0,0)},w_{(0,0,1)},w_{(1,0,0)},w_{(1,0,1)},w_{(2,0,0)},w_{(2,0,1)})^T
	\nonumber\\
H&=&\left(\begin{array}{cccccc}
	1&1&1&1&1&1\\
	1&-1&1&-1&1&-1\\
	1&1&e^{2 \pi\imath/3}&e^{2\pi\imath/3}&e^{4\pi\imath/3}&
		e^{4\pi\imath/3}\\
	1&-1&e^{2\pi\imath/3}&-e^{2\pi\imath/3}&e^{4\pi\imath/3}&
		-e^{4\pi\imath/3}\\
	1&1&e^{4\pi\imath/3}&e^{4\pi\imath/3}&e^{8 \pi\imath/3}&
		e^{8\pi\imath/3}\\
	1&-1&e^{4\pi\imath/3}&-e^{4\pi\imath/3}&e^{8 \pi\imath/3}&
		-e^{8\pi\imath/3}
	\end{array} \right) \nonumber\\
M&=&\left(\begin{array}{cccccc}
	0&2&0&2&0&2\\
	0&0&1-A &1-A&1-B&1-B\\
	0&2&1-A&1+A&1-B&1+B\\
	0&0& 1-B&1-B &1-A&1-A\\
	0&2&1-B&1+A&1-A&1+B
\end{array} \right)
\end{eqnarray}
where $A=e^{2 \pi \imath/3}$ and $B=e^{-2 \pi \imath/3}$.  $MW>0$ gives us
\begin{eqnarray}
2 w_{(0,0,1)} + 2 w_{(1,0,1)} + 2 w_{(2,0,1)} &>& 0 \nonumber\\
(1-A) w_{(1,0,0)} + (1-A) w_{(1,0,1)} + (1-B) w_{(2,0,0)}+(1-B) w_{(2,0,1)}&>&0
	\nonumber\\
2 w_{(0,0,1)} + (1-A) w_{(1,0,0)} +(1+A) w_{(1,0,1)} + (1-B) w_{(2,0,0)} +
	(1+B) w_{(2,0,1)} &>& 0 \nonumber\\
(1-B) w_{(1,0,0)} +(1-B)w_{(1,0,1)} + (1-A)w_{(2,0,0)}+(1-A)w_{(2,0,1)}&>&0
	\nonumber\\
2 w_{(0,0,1)} +(1-B) w_{(1,0,0)} +(1+B) w_{(1,0,1)} +(1-A) w_{(2,0,0)}+
	(1+A)w_{(2,0,1)} &>&0
\end{eqnarray}
Translated into function values, this gives us
\begin{eqnarray}
f((0,*,0)) &>& f((0,*,1)) \nonumber\\
f((0,*,0)) &>& f((1,*,0)) \nonumber\\
f((0,*,0)) &>& f((1,*,1)) \nonumber\\
f((0,*,0)) &>& f((2,*,0)) \nonumber\\
f((0,*,0)) &>& f((2,*,1))
\end{eqnarray}
which are the deceptive conditions corresponding to the competition
partition $(F,D,F)$.

For a function to be deceptive at order $p$ requires that all order $p$
schemas lead as far away to the global optimum as possible.  Since there
are $n \choose p$ ways of choosing $p$ fixed positions
among the $n$ characters, there are that many competition partitions at
that order.  In the above example, the function is deceptive at order 1 if
it is deceptive in the partitions (F,D,D), (D,F,D), and (D,D,F).
For full deception, the function must be deceptive at all orders between
1 and $n-1$ inclusive.  Mathematica routines for computing all the the
above are included in the appendix.

\section{The Variance of Fitness}
The schema theorem \cite{bible} by itself does not insure that the number of
individuals with that schema will grow, even initially, in a population
with finite size \cite{variance}.  With roulette wheel selection and an
infinite population, the number of copies an individual has in the target
population is proportional to its fitness.  With a finite population, however,
sampling noise inherent in the roulette wheel selection may increase or
decrease this number.  When we are calculating the growth rate of a
particular schema, it is important to keep in mind the statistical variance
in the growth rate as well.

One way to think of this process is to consider a single individual's
growth rate, then generalize the result to schemas.  Let $P(n,i)$ be
the probability that individual $i$ receives $n$ copies in the next
generation.  Then,
\begin{equation}
P(n,i)=p_i^n (1-p_i)^{N-n} {N \choose n}
\end{equation}
where $N$ is the population size and $p_i$ is the fitness of individual $i$
divided by the sum of the fitnesses of the entire population:
\begin{eqnarray}
p_i&=& \frac{1}{C} f_i \nonumber\\
C&=& \sum_{j=1}^N{f_j}
\end{eqnarray}
Now let $Q(f,s)$ be the probability distribution of fitness for schema $s$.
Consider a single individual taken at random from the set of individuals
covered by the schema $s$.
\begin{equation}
P(n,s)= \int_{-\infty}^{\infty}{
	df\, Q(f,s) (f/C)^n (1-f/C)^{N-n} {N \choose n}
}
\end{equation}
The variance in $n$ of $P(n,s)$ depends on the variance of $Q(f,s)$ about
its mean and the variance of $(f/C)^n (1-f/C)^{N-n}$ about its mean
at $f=n C/N= n \overline{f}$.
$n=f_s/\overline{f}$
\begin{eqnarray}
P(n,s) &\approx& \int_{-\infty}^{\infty}{
	   df\, e^{-(f-f_s)^2/2\sigma_s^2} e^{-n(f-N^2\overline{f})^2/n}}
	\nonumber\\
	&\approx& e^{-(f_s-n \overline{f})^2/(n/N^2 +2 \sigma_s^2)}
\end{eqnarray}
When we take all the individuals in the population that are covered by
schema $s$ and ask how many individuals in the target population are covered
by schema $s$
\begin{equation}
\sigma^2=\frac{1}{N_s}(\frac{n}{N^2}+2 \sigma_s^2)
\end{equation}

If $\sigma \gg N_s |1 - f_s/\overline{f}|$, then the noise dominates the
reproduction of the schema, and the growth rate of the schema is unpredictable.
Conversely, if $\sigma \ll N_s |1 - f_s /\overline{f}|$, then the noise is
negligible and the schema theorem applies.

In tournament selection, this problem manifests itself in another way.
Consider two schemas $s_1$ and $s_2$ with fitness probabilities
$Q(f,s_1)$ and $Q(f,s_2)$.  Let us choose one individual from each schema
and see which wins.  If the fitness probabilities $Q(f,s_1)$ and
$Q(f,s_2)$ have no variance, then the individual that comes from the
schema with the greater average fitness will always win.  Otherwise, the
probability that the individual from $s_2$ wins is
\begin{equation}
\int_{-\infty}^{\infty} {df_1
\int_{f_1}^{\infty} {df_2\,
Q(f_1,s_1) Q(f_2,s_2)}}
\end{equation}
Let us approximate the distribution of fitness as gaussians as before.
Without loss of generality, let us make the fitness of $s_1$ be zero
and the fitness of $s_2$ be $\Delta$.
\begin{eqnarray}
\int_{-\infty}^{\infty} {df_1
\int_{f_1}^{\infty} {df_2\,
Q(f_1,s_1) Q(f_2,s_2)}} &\approx&
\int_{-\infty}^{\infty} {df_1
\int_{f_1}^{\infty} {df_2\,
\frac{e^{-f_1^2/2\sigma_1^2}}{(2 \pi \sigma_1^2)^{1/2}}
\frac{e^{-(f_2-\Delta)^2/2\sigma_2^2}}{(2 \pi \sigma_2^2)^{1/2}}
}} \nonumber\\
&=& \int_{-\infty}^{\Delta}{dz\, \frac{e^{-z^2/2(\sigma_1^2+\sigma_2^2)}}{
	(2 \pi(\sigma_1^2+\sigma_2^2))^{1/2}}} \nonumber\\
&=& \int_{-\infty}^{\Delta/(\sigma_1^2+\sigma_2^2)^{1/2}}{dz\,
	\frac{e^{-z^2/2}}{(2\pi)^{1/2}}} \nonumber\\
&=& \frac{
	1+\mbox{erf}
	({\frac{\Delta}{(2(\sigma_1^2+\sigma_2^2))^{1/2}})}}{2}
\end{eqnarray}
So we see that with tournament selection, the variance of fitness actually
reduces the expected selection pressure instead of merely adding noise to it.
In both tournament selection and roulette wheel selection, the basic rule
of thumb is that in order to see whether noise the schema theorem dominates,
we compare the difference in fitness to the variance in fitness.  If this
signal-to-noise ratio is high, then the schema theorem is valid; otherwise
the noise dominates.

We can compute the variance of fitness for a schema $s$ in the following
manner:
\begin{eqnarray}
\mbox{var}(f(s))&=&\frac{1}{N_s} \sum{\left(f(x)-\frac{1}{N_s}
	\sum{f(y)}\right)^2}\nonumber\\
&=& \frac{1}{N_s} \sum{f(x)^2} -\left(\frac{1}{N_s} \sum{f(x)}\right)^2
	\nonumber\\
&=& <f^2>_s-<f>_s^2
\end{eqnarray}
where $<y>_s$ indicates the expectation of $y$ over schema $s$.
In a previous section, we had some examples of computing $<f^2>$, and
now we will generalize those examples to state that:
\begin{equation}
\mbox{var}(f(s))=\frac{1}{k_1 k_2 \ldots k_n}\sum_{(i,j) \epsilon J^2_{\ominus}(h)-J^2(h)}{
	\overline{w_i} w_j \Psi_{j \ominus i}(h)}
\end{equation}
where $J(h)$ is the similarity subset $(J:\{0,1,*\}^n \rightarrow \{0,1\}^n)$
created by replacing *s by 0s and fixed positions by *s:
\begin{equation}
J_i(h)=\left\{\begin{array}{ll}
	0,&\mbox{if $h_i$=*;}\\
	**,&\mbox{otherwise}
	\end{array}\right.
\end{equation}
$J^2(h)=J(h) \times J(h)$, $J^2_{\ominus}=\{(i,j):j\ominus i \epsilon J(h)
\}$, and $j \ominus i=(j_1-i_1 \mbox{mod} k_1,j_2-i_2 \mbox{mod} k_2,\ldots,
	j_n-i_n \mbox{mod} k_n)$.
The details of the derivation for the binary strings are given in {variance}.

Although in general, computing the variance of a schema with Walsh functions
requires more operations than computing the variance using the fitness
function directly, in certain cases there are shortcuts that make it more
efficient.  For example, for relatively short strings, large alphabets,
and fairly smooth fitness functions, we can use the alphabet-size reduction
described earlier to get a good approximation to the variances with
relatively little effort.  For instance, the variance of Test Function 1 is:
\begin{equation}
\int_0^1{(x^2 (1-x)^2 (1/2-x)^3)^2 dx}-\left(\int_0^1{(x^2(1-x)^2(1/2-x)^3) dx}
	\right)^2=1.734\times 10^{-7}
\end{equation}
We can approximate the same calculation using the 7-ary reduction:
\begin{equation}
\frac{1}{7}(|w_1|^2+|w_2|^2+|w_3|^2+|w_4|^2+|w_5|^2+|w_6|^2)=1.723\times 10^{-7}
\end{equation}

\section{Conclusion}
only one method of analyzing deception, there may be others
examples of time evolution constrained by scaling symmetry

\appendix
\section{Appendix -- {\em Mathematica 2.0\/} Programs}


\begin{verbatim}
Generalized Walsh Functions Over k-ary Strings
----------------------------------------------
k is the size of the alphabet
n is the maximum length of the strings
j is the index (in integer form) of the generalized Walsh function
x is the argument (in integer form) of the generalized Walsh function

Psi[k_Integer,n_Integer,j_Integer,x_Integer]:=
	E^(2 Pi I IntegerDigits[j,k,n].IntegerDigits[x,k,n] / k)/
	Sqrt[k^n]

Demonstration:
From the ternary alphabet and strings of length two example in the text
above, we evaluate the fourth Walsh function at position two.


In[3]:= Psi[3,2,4,2]

         (-2 I)/3 Pi
        E
Out[3]= ------------
             3


Generalized Walsh Functions Over More Arbitrary Strings
-------------------------------------------------------
k is a list of the sizes of the alphabet used in each character
j is the index (in vector form) of the generalized Walsh function
x is the argument (in vector form) of the generalized Walsh function

Psi[k_List,j_List,x_List]:=
	E^(2 Pi I Plus @@ MapThread[#1 #2/#3 &, {j, x, k}])/
	Sqrt[Times@@k]

Demonstration:
Let us consider the set of strings with length two.  The first character
is chosen from a binary alphabet, and the second from a ternary alphabet.
Let us evaluate the (0,2) Walsh function at position (1,2).


In[6]:= Psi[{2,3},{0,2},{1,2}]

         (2 I)/3 Pi
        E
Out[6]= -----------
          Sqrt[6]


Generalized Fast Walsh Transform Over k-ary Strings
---------------------------------------------------
k is the size of the alphabet
l is the list of function values whose length is a power of k

GFWT[k_Integer,l_List]:=
	Flatten[
	Nest[ Function[z,Table[E^-(2 Pi I j x/k),{j,0,k-1},{x,0,k-1}].z] /@
		Partition[#,k]&,l,Round[Log[k,Length[l]]]]]/
			Sqrt[k^Round[Log[k,Length[l]]]]

InverseGFWT[k_Integer,l_List]:=
	Flatten[
	Nest[ Function[z,Table[E^(2 Pi I j x/k),{j,0,k-1},{x,0,k-1}].z] /@
		Partition[#,k]&,l,Round[Log[k,Length[l]]]]]/
			Sqrt[k^Round[Log[k,Length[l]]]]

Demonstration:
We create a list of 9 random numbers, find the transform for a ternary
alphabet, and then untransform the data to recover the original 9 numbers.

In[3]:= Table[Random[],{9}]

Out[3]= {0.177361, 0.503785, 0.387824, 0.615398, 0.254133, 0.488114, 
 
>    0.00850412, 0.539387, 0.788743}

In[4]:= GFWT[3,%] //N    

Out[4]= {1.25442, -0.226577 + 0.106052 I, -0.226577 - 0.106052 I, 
 
>    -0.0927234 - 0.00606561 I, -0.0247775 - 0.362999 I, 
 
>    -0.0170897 - 0.156521 I, -0.0927234 + 0.00606561 I, 
 
>    -0.0170897 + 0.156521 I, -0.0247775 + 0.362999 I}

In[5]:= InverseGFWT[3,%] //N //Chop

Out[5]= {0.177361, 0.503785, 0.387824, 0.615398, 0.254133, 0.488114, 
 
>    0.00850412, 0.539387, 0.788743}



Generalized Fast Walsh Transform Over More Arbitrary Strings
------------------------------------------------------------
k is a list of the sizes of the alphabets in the string
l is the list of function values whose length is a the product of 
     the sizes of the alphabets

GFWT[k_List,l_List]:=
	Flatten[Fold[
	Function[z, Table[E^-(2 Pi I i j/#2),{i,0,#2-1},{j,0,#2-1}] . z] /@
		Partition[#1,#2] &, l, Reverse[k]]]/
			Sqrt[Times@@k]

InverseGFWT[k_List,l_List]:=
	Flatten[Fold[
	Function[z, Table[E^(2 Pi I i j/#2),{i,0,#2-1},{j,0,#2-1}] . z] /@
		Partition[#1,#2] &, l, Reverse[k]]]/
			Sqrt[Times@@k]


Demonstration:
We create a list of 6 random numbers and transform it over strings of
length 2 whose first character is chosen from a binary alphabet and
whose second character is chosen from a ternary alphabet.

In[3]:= Table[Random[],{6}]

Out[3]= {0.222531, 0.656238, 0.587434, 0.827656, 0.34676, 0.514834}

In[4]:= GFWT[{2,3},%] //N

Out[4]= {1.28821, -0.000998969 + 0.0350975 I, -0.000998969 - 0.0350975 I, 
 
>    -0.0910589, -0.325032 - 0.0837492 I, -0.325032 + 0.0837492 I}

In[5]:= InverseGFWT[{2,3},%] //N //Chop

Out[5]= {0.222531, 0.656238, 0.587434, 0.827656, 0.34676, 0.514834}


Converting Schema Averages to Walsh Coefficients
------------------------------------------------
k is a list containing the sizes of the alphabets
schema is the schema which we want to evaluate the fitness of.
	D is the don't-care symbol
w is the name of the generalized Walsh coefficients

SchemaToWalsh[k_List,schema_List,w_]:=
        Sum @@
        Prepend[
        Delete[MapIndexed[{j[#2[[1]]],0,#1-1}&,k],Position[schema,D]],
        w[(j[#]&/@Range[Length[k]]).
                Reverse[FoldList[Times,1,Reverse[Rest[k]]]]]
        E^(2 Pi I Plus @@ MapIndexed[j[#2[[1]]] schema[[#2[[1]]]]/#1 &,k])
        /. (j[#] ->0 &/@ Flatten[Position[schema,D]])
        ] / Sqrt[Times@@k]


Demonstration:
We consider strings of length 3 whose characters are taken from alphabets
with cardinality 2, 4, and 2.  We calculate several schema averages in terms 
of the generalized Walsh coefficients.


In[14]:= SchemaToWalsh[{2,4,2},{D,D,D},w]

         w[0]
Out[14]= ----
          4

In[15]:= SchemaToWalsh[{2,4,2},{0,D,D},w]

         w[0] + w[8]
Out[15]= -----------
              4

In[16]:= SchemaToWalsh[{2,4,2},{1,D,D},w]

         w[0] - w[8]
Out[16]= -----------
              4

In[17]:= SchemaToWalsh[{2,4,2},{0,0,D},w]

         w[0] + w[2] + w[4] + w[6] + w[8] + w[10] + w[12] + w[14]
Out[17]= --------------------------------------------------------
                                    4

In[18]:= SchemaToWalsh[{2,4,2},{D,0,D},w]

         w[0] + w[2] + w[4] + w[6]
Out[18]= -------------------------
                     4


Hadamard Transforms
-------------------

(* tensor[a,b] returns the tensor produce of a and b *)
tensor[a_,b_]:=With[{s1=Length[a],s2=Length[b]},
        Table[a[[Ceiling[i/s2],Ceiling[j/s2]]]
                b[[Mod[i-1,s2]+1,Mod[j-1,s2]+1]],{i,s1 s2},{j,s1 s2}]]

(* returns a Hadamard matrix for a single character *)
h[k_]:= Table[E^(2 Pi I i j/k),{i,0,k-1},{j,0,k-1}]

(* returns a Hadamard matrix for a set of characters *)
H[k_List]:= Fold[tensor[#1,#2]&,h[First[k]],h/@Rest[k]]

(* returns the matrix for finding deceptive conditions *)
M[k_List]:= With[{hmat=H[k]},
        Table[hmat[[1]]-hmat[[i]],{i,2,Length[hmat]}]]


Determining Deception Using Hadamard Transforms
-----------------------------------------------
(* returns a list of Walsh coefficients in the partition specified by the *)
(* schema *)
WalshList[k_List,schema_List,w_]:=
        Flatten[
        Table @@
        Prepend[
        Delete[MapIndexed[{j[#2[[1]]],0,#1-1}&,k],Position[schema,D]],
        w[(j[#]&/@Range[Length[k]]).
                Reverse[FoldList[Times,1,Reverse[Rest[k]]]]]
        /. (j[#] ->0 &/@ Flatten[Position[schema,D]])
        ]]

(* returns the Walsh sums needed for the deceptive conditions *)
(* F=fixed position, D = don't care *)
Decep[k_List,schema_List,w_]:=
        M[k[[#[[1]]]]& /@ Position[schema,F]].WalshList[k,schema,w]


Demonstration:
We consider a function over a (3,2,2)-ary alphabet and the competition
partition (F,D,D).  Decep[{3,2,2},{F,D,D},w] returns a vector whose components
must all be positive for the function to be deceptive.

In[3]:= Decep[{3,2,2},{F,D,D},w]

               (2 I)/3 Pi               (-2 I)/3 Pi
Out[3]= {(1 - E          ) w[4] + (1 - E           ) w[8], 
 
           (-2 I)/3 Pi               (2 I)/3 Pi
>    (1 - E           ) w[4] + (1 - E          ) w[8]}


Demonstration:
Now we consider the above function over the competition partition (F,F,D).

In[4]:= Decep[{3,2,2},{F,F,D},w]

Out[4]= {2 w[2] + 2 w[6] + 2 w[10], 
 
           (2 I)/3 Pi               (2 I)/3 Pi
>    (1 - E          ) w[4] + (1 - E          ) w[6] + 
 
            (-2 I)/3 Pi               (-2 I)/3 Pi
>     (1 - E           ) w[8] + (1 - E           ) w[10], 
 
                    (2 I)/3 Pi               (2 I)/3 Pi
>    2 w[2] + (1 - E          ) w[4] + (1 + E          ) w[6] + 
 
            (-2 I)/3 Pi               (-2 I)/3 Pi
>     (1 - E           ) w[8] + (1 + E           ) w[10], 
 
           (-2 I)/3 Pi               (-2 I)/3 Pi
>    (1 - E           ) w[4] + (1 - E           ) w[6] + 
 
            (2 I)/3 Pi               (2 I)/3 Pi
>     (1 - E          ) w[8] + (1 - E          ) w[10], 
 
                    (-2 I)/3 Pi               (-2 I)/3 Pi
>    2 w[2] + (1 - E           ) w[4] + (1 + E           ) w[6] + 
 
            (2 I)/3 Pi               (2 I)/3 Pi
>     (1 - E          ) w[8] + (1 + E          ) w[10]}

Determining Deception to Specified Order
----------------------------------------
GenDecep[k_List,order_,w_]:=
        Flatten[
        Decep[k,#,w]& /@
        Permutations[Join[Table[F,{order}],Table[D,{Length[k]-order}]]]]

Demonstration:
Consider the function used in the previous example.  In order for the
function to be deceptive at the 1-st order, all the components of
GenDecep[{3,2,2},1,w] must be positive.

In[5]:= GenDecep[{3,2,2},1,w]

               (2 I)/3 Pi               (-2 I)/3 Pi
Out[5]= {(1 - E          ) w[4] + (1 - E           ) w[8], 
 
           (-2 I)/3 Pi               (2 I)/3 Pi
>    (1 - E           ) w[4] + (1 - E          ) w[8], 2 w[2], 2 w[1]}


Fully Deceptive Conditions
--------------------------
FullDecep[k_List,w_]:=
        Flatten[Table[GenDecep[k,q,w],{q,1,Length[k]-1}]]


Demonstration:
Consider again a function over strings taken from a (3,2,2)-ary alphabet.
Then the conditions for full deception are that each of the components
of the vector returned by FullDecep[{3,2,2},w] are positive.

In[6]:= FullDecep[{3,2,2},w]

               (2 I)/3 Pi               (-2 I)/3 Pi
Out[6]= {(1 - E          ) w[4] + (1 - E           ) w[8], 
 
           (-2 I)/3 Pi               (2 I)/3 Pi
>    (1 - E           ) w[4] + (1 - E          ) w[8], 2 w[2], 2 w[1], 
 
>    2 w[2] + 2 w[6] + 2 w[10], 
 
           (2 I)/3 Pi               (2 I)/3 Pi
>    (1 - E          ) w[4] + (1 - E          ) w[6] + 
 
            (-2 I)/3 Pi               (-2 I)/3 Pi
>     (1 - E           ) w[8] + (1 - E           ) w[10], 
 
                    (2 I)/3 Pi               (2 I)/3 Pi
>    2 w[2] + (1 - E          ) w[4] + (1 + E          ) w[6] + 
 
            (-2 I)/3 Pi               (-2 I)/3 Pi
>     (1 - E           ) w[8] + (1 + E           ) w[10], 
 
           (-2 I)/3 Pi               (-2 I)/3 Pi
>    (1 - E           ) w[4] + (1 - E           ) w[6] + 
 
            (2 I)/3 Pi               (2 I)/3 Pi
>     (1 - E          ) w[8] + (1 - E          ) w[10], 
 
                    (-2 I)/3 Pi               (-2 I)/3 Pi
>    2 w[2] + (1 - E           ) w[4] + (1 + E           ) w[6] + 
 
            (2 I)/3 Pi               (2 I)/3 Pi
>     (1 - E          ) w[8] + (1 + E          ) w[10], 
 
                                     (2 I)/3 Pi
>    2 w[1] + 2 w[5] + 2 w[9], (1 - E          ) w[4] + 
 
            (2 I)/3 Pi               (-2 I)/3 Pi
>     (1 - E          ) w[5] + (1 - E           ) w[8] + 
 
            (-2 I)/3 Pi                       (2 I)/3 Pi
>     (1 - E           ) w[9], 2 w[1] + (1 - E          ) w[4] + 
 
            (2 I)/3 Pi               (-2 I)/3 Pi
>     (1 + E          ) w[5] + (1 - E           ) w[8] + 
 
            (-2 I)/3 Pi              (-2 I)/3 Pi
>     (1 + E           ) w[9], (1 - E           ) w[4] + 
 
            (-2 I)/3 Pi               (2 I)/3 Pi
>     (1 - E           ) w[5] + (1 - E          ) w[8] + 
 
            (2 I)/3 Pi                       (-2 I)/3 Pi
>     (1 - E          ) w[9], 2 w[1] + (1 - E           ) w[4] + 
 
            (-2 I)/3 Pi               (2 I)/3 Pi
>     (1 + E           ) w[5] + (1 - E          ) w[8] + 
 
            (2 I)/3 Pi
>     (1 + E          ) w[9], 2 w[1] + 2 w[3], 2 w[2] + 2 w[3], 
 
>    2 w[1] + 2 w[2]}


\end{verbatim}

\begin{thebibliography}{9}
\bibitem{bible} Goldberg, D. E., {\em Genetic Algorithms in Search,
	Optimization, and Machine Learning}, Addison-Wesley, 1989.
\bibitem{real} Goldberg, D. E., ``Real-Coded Genetic Algorithms,
	Virtual Alphabets, and Blocking'', IlliGAL Report 90001, September
	1990.
\bibitem{hadamard} Homaifar, A., Qi, X., Fost, J., ``Analysis and
	Design of a General GA Deceptive Problem'', Proceedings of the
	Fourth International Conference on Genetic Algorithms, pp. 196-203,
	1991.
\bibitem{variance} Goldberg, D. E., Rudnick, M., ``Genetic Algorithms
	and the Variance of Fitness'', Complex Systems, Vol. 5, pp. 265-278,
	1991.
\end{thebibliography}


\end{document}

